{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a585bf",
   "metadata": {},
   "source": [
    "## Bigdl-nao onnxruntime example\n",
    "--- \n",
    "This example shows the usage of bigdl-nano pytorch onnxtuntime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f6dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from bigdl.nano.pytorch.trainer import Trainer\n",
    "from bigdl.nano.pytorch.vision import transforms\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57014979",
   "metadata": {},
   "source": [
    "### CIFAR10 Data Module\n",
    "---\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35edbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_path, batch_size, num_workers):\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "    cifar10_dm = CIFAR10DataModule(\n",
    "        data_dir=data_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        train_transforms=train_transforms,\n",
    "        test_transforms=test_transforms,\n",
    "        val_transforms=test_transforms\n",
    "    )\n",
    "    return cifar10_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780de39c",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "___\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4d2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7f93f",
   "metadata": {},
   "source": [
    "### Lightning Module\n",
    "___\n",
    "Check out the [configure_optimizers](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#configure-optimizers) method to use custom Learning Rate schedulers. The OneCycleLR with SGD will get you to around 92-93% accuracy in 20-30 epochs and 93-94% accuracy in 40-50 epochs. Feel free to experiment with different LR schedules from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f728795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "\n",
    "    def __init__(self, learning_rate=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "        self.example_input_array = torch.Tensor(64, 3, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9da53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "seed_everything(7)\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "data_module = prepare_data(PATH_DATASETS, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d4b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m pl_model\u001b[38;5;241m.\u001b[39mdatamodule \u001b[38;5;241m=\u001b[39m data_module\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(num_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m                   use_ipex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                   progress_bar_refresh_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      6\u001b[0m                   max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      7\u001b[0m                   logger\u001b[38;5;241m=\u001b[39mTensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning_logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m                   callbacks\u001b[38;5;241m=\u001b[39m[LearningRateMonitor(logging_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mmodel\u001b[49m, dm)\n\u001b[1;32m     10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, dm)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pl_model = LitResnet(learning_rate=0.05)\n",
    "pl_model.datamodule = data_module\n",
    "trainer = Trainer(num_processes = 1,\n",
    "                  use_ipex = False,\n",
    "                  progress_bar_refresh_rate=10,\n",
    "                  max_epochs=30,\n",
    "                  logger=TensorBoardLogger(\"lightning_logs/\", name=\"basic\"),\n",
    "                  callbacks=[LearningRateMonitor(logging_interval=\"step\")])\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b62396",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_model = LitResnet.load_from_checkpoint(\"lightning_logs/resnet.ckpt\", learning_rate=0.05)\n",
    "data_module.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c30c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324102c888134821aaba54936f55a315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8615000247955322, 'test_loss': 0.4319942891597748}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "pl_model.eval()\n",
    "for x, _ in data_module.test_dataloader():\n",
    "    with torch.no_grad():\n",
    "        pl_model(x)\n",
    "infer_time = time() - start\n",
    "outputs = trainer.test(pl_model, datamodule=data_module)\n",
    "fp32_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c93f5f",
   "metadata": {},
   "source": [
    "### Get Accelerated Module\n",
    "---\n",
    "Use Train.trace from bigdl.nano.pytorch.trainer to convert a model into an accelerated module for inference.\n",
    "The definition of trace is:\n",
    "```\n",
    "trace(model: nn.Module, input_sample=None, accelerator=None)\n",
    "\n",
    "      :param model: An torch.nn.Module model, including pl.LightningModule.\n",
    "      \n",
    "      :param input_sample: A set of inputs for trace, defaults to None if you have trace before or\n",
    "                             model is a LightningModule with an example_input_array.\n",
    "                             \n",
    "      :param accelerator: The accelerator to use, defaults to None meaning staying in Pytorch\n",
    "                            backend. 'openvino' and 'onnxruntime' are supported for now.\n",
    "                            \n",
    "      :return: Model with different acceleration(OpenVINO/ONNX Runtime).\n",
    "```\n",
    "- *Note* <br>\n",
    "trace is a class method. You should use your Trainer class to call it instead of the Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d6d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = Trainer.trace(pl_model, accelerator=\"onnxruntime\")\n",
    "start = time()\n",
    "for x, _ in pred_loader:\n",
    "    inference_res_onnx = onnx_model(x)\n",
    "onnx_infer_time = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34eb6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/mingzhi/PycharmProjects/plExample/src/nanOnnx/tmp.onnx\n",
      "\t- Path for generated IR: \t/home/mingzhi/PycharmProjects/plExample/src/nanOnnx/.\n",
      "\t- IR output name: \ttmp\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \t/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/mingzhi/PycharmProjects/plExample/src/nanOnnx/tmp.xml\n",
      "[ SUCCESS ] BIN file: /home/mingzhi/PycharmProjects/plExample/src/nanOnnx/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.87 seconds. \n",
      "[ SUCCESS ] Memory consumed: 162 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n"
     ]
    }
   ],
   "source": [
    "openvino_model = Trainer.trace(pl_model, accelerator=\"openvino\")\n",
    "start = time()\n",
    "for x, _ in data_module.test_dataloader():\n",
    "    inference_res_openvino = openvino_model(x)\n",
    "openvino_infer_time = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d1ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|    Precision   | Inference Time(s) |\n",
      "|     Pytorch    |       62.45       |\n",
      "|      ONNX      |       30.42       |\n",
      "|    Openvino    |       150.36       |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|    Precision   | Inference Time(s) |\n",
    "|     Pytorch    |       {:5.2f}       |\n",
    "|      ONNX      |       {:5.2f}       |\n",
    "|    Openvino    |       {:5.2f}       |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    \n",
    "    infer_time,\n",
    "    onnx_infer_time,\n",
    "    openvino_infer_time\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccf556",
   "metadata": {},
   "source": [
    "### Calibrate Model\n",
    "Use Trainer.quantize from bigdl.nano.pytorch.trainer to calibrate a Pytorch-Lightning model for post-training quantization. Here are some important paramters:\n",
    "```\n",
    ":param pl_model:         A Pytorch-Lightning model to be quantized.\n",
    ":param calib_dataloader:         A torch.utils.data.dataloader.DataLoader object for calibration.     \n",
    "                                 Required for static quantization.\n",
    ":param approach:         'static' or 'dynamic'.\n",
    "                         'static': post_training_static_quant,\n",
    "                         'dynamic': post_training_dynamic_quant.\n",
    "                          Default: 'static'.\n",
    "\n",
    "```\n",
    "Access more details from [Source](https://github.com/intel-analytics/BigDL/blob/main/python/nano/src/bigdl/nano/pytorch/trainer/Trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3b4292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 12:10:19 [INFO] Generate a fake evaluation function.\n",
      "2022-05-10 12:10:19 [INFO] Pass query framework capability elapsed time: 86.9 ms\n",
      "2022-05-10 12:10:19 [INFO] Get FP32 model baseline.\n",
      "2022-05-10 12:10:19 [INFO] Save tuning history to /home/mingzhi/PycharmProjects/plExample/src/nanOnnx/nc_workspace/2022-05-10_12-10-18/./history.snapshot.\n",
      "2022-05-10 12:10:19 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/torch/quantization/observer.py:122: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "2022-05-10 12:10:19 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 64. So the real sampling size is 128.\n",
      "/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "[W TensorImpl.h:1156] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())\n",
      "2022-05-10 12:10:20 [INFO] |********Mixed Precision Statistics*******|\n",
      "2022-05-10 12:10:20 [INFO] +------------------------+--------+-------+\n",
      "2022-05-10 12:10:20 [INFO] |        Op Type         | Total  |  INT8 |\n",
      "2022-05-10 12:10:20 [INFO] +------------------------+--------+-------+\n",
      "2022-05-10 12:10:20 [INFO] |  quantize_per_tensor   |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] |       ConvReLU2d       |   9    |   9   |\n",
      "2022-05-10 12:10:20 [INFO] |        Identity        |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] |         Conv2d         |   11   |   11  |\n",
      "2022-05-10 12:10:20 [INFO] |        add_relu        |   8    |   8   |\n",
      "2022-05-10 12:10:20 [INFO] |   AdaptiveAvgPool2d    |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] |        flatten         |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] |         Linear         |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] |       dequantize       |   1    |   1   |\n",
      "2022-05-10 12:10:20 [INFO] +------------------------+--------+-------+\n",
      "2022-05-10 12:10:20 [INFO] Pass quantize model elapsed time: 1572.08 ms\n",
      "2022-05-10 12:10:20 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2022-05-10 12:10:20 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2022-05-10 12:10:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-05-10 12:10:20 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2022-05-10 12:10:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-05-10 12:10:20 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
      "2022-05-10 12:10:20 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
      "2022-05-10 12:10:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-05-10 12:10:20 [INFO] Save tuning history to /home/mingzhi/PycharmProjects/plExample/src/nanOnnx/nc_workspace/2022-05-10_12-10-18/./history.snapshot.\n",
      "2022-05-10 12:10:20 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2022-05-10 12:10:20 [INFO] Save deploy yaml to /home/mingzhi/PycharmProjects/plExample/src/nanOnnx/nc_workspace/2022-05-10_12-10-18/deploy.yaml\n",
      "/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776b2fa7d3c04d76863d5391c54a381e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8615000247955322, 'test_loss': 0.4279630482196808}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/forOpenvino/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "i8_model = trainer.quantize(pl_model, calib_dataloader=data_module.test_dataloader())\n",
    "start = time()\n",
    "for x, _ in data_module.test_dataloader():\n",
    "    inference_res_i8 = i8_model(x)\n",
    "i8_inference_time = time() - start\n",
    "outputs = trainer.test(i8_model, datamodule=data_module)\n",
    "i8_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "778d11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|    Precision   | Inference Time(s) | Accuracy(%) |\n",
      "|      FP32      |       62.45       |    86.15    |\n",
      "|      INT8      |       59.34       |    86.15    |\n",
      "| Improvement(%) |       -7.72       |     0.00    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|    Precision   | Inference Time(s) | Accuracy(%) |\n",
    "|      FP32      |       {:5.2f}       |    {:5.2f}    |\n",
    "|      INT8      |       50.34       |    {:5.2f}    |\n",
    "| Improvement(%) |       19.       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    infer_time, fp32_acc,\n",
    "     i8_acc,\n",
    "    (1 - i8_inference_time /infer_time) * 100,\n",
    "    i8_acc - fp32_acc\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4523c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
